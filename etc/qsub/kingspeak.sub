#!/bin/bash
#SBATCH --time=%(wall_time_hrs)02d:00:00 # walltime, abbreviated by -t
#SBATCH --nodes=%(nodes)d     # number of cluster nodes, abbreviated by -N
##SBATCH -o  # name of the stdout
#SBATCH --ntasks=%(np)d   # number of MPI tasks, abbreviated by -n
# additional information for allocated clusters
#SBATCH --account=kochanski-kp     # account - abbreviated by -A
#SBATCH --partition=strong-kp  # partition, abbreviated by -p
#SBATCH --job-name=WRFX
# set data and working directories
export WORKDIR=%(cwd)s 
cd $WORKDIR
#
# load appropriate modules, in this case Intel compilers, MPICHa
module purge
module load chpc
module load intel/18 
module load impi
module load netcdf-c
module load netcdf-f
# for MPICH2 over Ethernet, set communication method to TCP
# see below for network interface selection options for different MPI distributions
export MV2_ENABLE_AFFINITY=0
env
#setenv MPICH_NEMESIS_NETMOD tcp
# run the program
# see below for ways to do this for different MPI distributions
mpirun -np $SLURM_NTASKS ./wrf.exe
